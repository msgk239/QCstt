# 语音识别文本纠正方案

## 模块状态
当前模块已完成开发和优化，具备以下特点：
1. 功能完备：
   - 支持原词直接替换和相似度匹配
   - 支持上下文条件判断
   - 支持自定义阈值和词典配置
   - 保持语音识别的标记和时间戳

2. 性能稳定：
   - 2小时语音处理仅需4秒
   - 内存占用合理
   - 无需多线程优化

3. 易于维护：
   - 配置方式简单（keywords文件）
   - 日志分级清晰
   - 代码结构完善

4. 后续建议：
   - 定期更新keywords词库
   - 根据实际效果调整阈值
   - 收集用户反馈优化准确率

## 目标
对ASR识别结果进行后处理，将错误识别的词纠正为预设的目标词。

## 技术方案
分三个阶段处理：原词映射、分词处理、相似度匹配

1. 预处理阶段：
   - 从 keywords 文件加载目标词表
   - 建立原词到目标词的映射关系
   - 将目标词转换为拼音表示
   - 支持多音字处理
   - 初始化pkuseg分词器（单例模式）

2. 处理流程：
   a. 原词映射（第一优先级）：
      - 直接在文本中查找原词
      - 使用最长匹配优先原则
      - 记录已替换位置，避免重复处理
      - 支持上下文条件判断
      
   b. 分词处理（未匹配文本）：
      - 使用pkuseg进行分词
      - 优势：
        * 原始分词准确率更高
        * 对专业术语识别更准确
        * 使用序列标注模型，考虑上下文语义
      
   c. 相似度匹配（分词结果）：
      - 对分词结果进行拼音转换
      - 计算与目标词的相似度
      - 根据阈值进行纠正

3. 优化策略：
   - 预先排序原词列表（长词优先）
   - 使用位置记录避免重叠替换
   - 支持单词级别的自定义阈值
   - 缓存常见词的拼音结果
   - 自动同步配置文件

## 配置方式
1. keywords 文件格式：
```txt
本源
检测点 0.75 # 可选：指定单独的阈值
心智控制区
第一灵 0.8 第一名,第1名 # 数字为阈值，后面是原词列表
整场 0.8 整仓,城产 (互催,互催下来) # 带上下文词要求的配置
```

2. 自动生成的 yaml 配置：
```yaml
target_words:
  本源:
    pinyin: [ben3, yuan2]
    context_words: []  # 无上下文要求
    similarity_threshold: 0.6 # 默认阈值
    original_words: []  # 原词列表
  第一灵:
    pinyin: [di4, yi1, ling2]
    context_words: []  # 无上下文要求
    similarity_threshold: 0.8 # 自定义阈值
    original_words: [第一名, 第1名]  # 原词列表
  整场:
    pinyin: [zheng3, chang3]
    context_words: [互催, 互催下来]  # 有上下文要求
    similarity_threshold: 0.8
    original_words: [整仓, 城产]  # 原词列表
```

## 使用说明
- 直接编辑 keywords 文件添加/删除目标词
- 可在词后添加数字来设置单独的阈值（0-1之间）
- 可在阈值后添加原词列表，使用逗号分隔（中文逗号"，"和英文逗号","都可以）
- 可以用括号指定上下文词要求：(上下文词1,上下文词2)
- 不指定阈值时使用默认值 0.6
- yaml 配置文件会自动更新

## 处理优先级说明
1. 原词匹配（最高优先级）：
   - 直接匹配原词列表中的词
   - 不考虑相似度和上下文要求
   - 例如："整仓" -> "整场"，不需要检查上下文
   - 最长匹配优先，避免短词替换破坏长词
   - 记录已替换位置，避免重复处理

2. 相似度匹配（次优先级）：
   - 对未被原词匹配的文本：
     1. 先进行分词
     2. 对分词结果进行相似度匹配
   - 无上下文要求的目标词：
     * 只检查拼音相似度是否达到阈值
     * 例如："第一令" -> "第一灵"
   - 有上下文要求的目标词：
     * 必须同时满足两个条件：
       1. 拼音相似度达到阈值
       2. 文本中包含指定的上下文词
     * 例如："正常" -> "整场" 只在有"互催"等上下文词时才会发生

## 分词器说明
1. 单例模式实现：
```python
class TextCorrector:
    _instance = None
    _segmenter = None
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
        
    def __init__(self, config_file: str = "correction_config.yaml"):
        if self._segmenter is None:
            # 只在第一次初始化分词器
            self._segmenter = pkuseg.pkuseg(user_dict='custom_dict.txt')
```

2. 性能特征：
   - 初始化时间：约1.8秒（仅首次）
   - 分词速度：
     * 短文本（100字以内）：<0.001秒
     * 长文本（1万字）：约0.17秒
   - 内存占用：比jieba略大

3. 使用建议：
   - 使用单例模式避免重复初始化
   - 在服务启动时预加载模型
   - 定期更新自定义词典
   - 对大规模文本考虑使用进程池

## 语音识别结果处理
1. 输入格式说明：
   ```python
   {
     'key': 'rand_key_xxx',
     'text': '<|zh|><|NEUTRAL|><|Speech|><|withitn|>第一句。         <|zh|><|NEUTRAL|><|Speech|><|withitn|>第二句。',
     'timestamp': [[0, 90], [90, 270], ...],
     'sentence_info': [
       {
         'start': 0,
         'end': 1180,
         'sentence': '<|zh|><|NEUTRAL|><|Speech|><|withitn|>第一句。',
         'timestamp': [[0, 90], ...],
         'spk': 0
       },
       ...
     ]
   }
   ```

2. 处理流程：
   - 遍历 sentence_info 中的每个句子
   - 提取每句中4个标记(`<|...|>`)后的纯文本
   - 先进行原词替换
   - 对未匹配部分进行分词和相似度匹配
   - 保持原有标记，只替换纯文本部分
   - 使用原格式拼接所有句子重建总文本

3. 格式保持：
   - 保持所有时间戳信息
   - 保持说话人标记
   - 保持情感等标记
   - 保持原有的缩进和分隔格式
   - 保持单引号包裹的格式

## 注意事项
- 原词列表应包含常见的错误识别形式
- 较长的词组应优先配置
- 上下文词的选择要具有代表性
- 定期根据实际效果更新原词列表和相似度阈值
- 避免原词之间的冲突或循环替换

## 测试命令
```bash
python -m server.api.speech.test_correction
python -m server.api.speech.test_correct_recognition
```

## 性能说明
1. 处理耗时：
   - 短文本（100字以内）：约100-200毫秒
   - 长文本（2小时语音）：约4秒
   - 平均每分钟语音处理耗时：约0.033秒

2. 性能优化策略：
   - 拼音结果缓存：使用`pinyin_cache`缓存词语的拼音结果
   - 词典优化：按长度排序，长词优先匹配
   - 位置记录：使用`replaced_positions`避免重复处理
   - 相似度计算优化：
     * 仅比较相同长度的词
     * 有上下文要求时先检查上下文
     * 达到阈值才更新最佳匹配

3. 日志优化：
   - 详细日志使用debug级别：分词、相似度匹配过程
   - 关键信息使用info级别：原词替换、最终结果
   - 耗时超过1秒时自动切换到秒显示
   - 仅在发生纠正时才输出最终结果

4. 无需优化的部分：
   - 当前单线程处理性能已经满足需求
   - 分词器性能稳定，无需多线程
   - 内存占用合理，无需特别优化
   - 处理延迟相比语音输入时间可以忽略

5. 监控指标：
   - 总体处理耗时
   - 分词耗时
   - 相似度匹配次数
   - 成功纠正次数
   - 词典大小统计